{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9b62c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] meta  saved: data/parquet/core30_meta.parquet   rows=30\n",
      "[OK] prices saved: data/parquet/core30_prices_1y_1d.parquet rows=7350\n",
      "[OK] manifest updated: data/parquet/manifest.json\n",
      "[OK] uploaded: s3://dash-plotly/parquet/core30_meta.parquet\n",
      "[OK] uploaded: s3://dash-plotly/parquet/core30_prices_1y_1d.parquet\n",
      "[OK] uploaded: s3://dash-plotly/parquet/manifest.json\n"
     ]
    }
   ],
   "source": [
    "# === TOPIX Core 30 の株価(1y,1d)を取得して Parquet に保存 + manifest更新 + S3アップロード ===\n",
    "from pathlib import Path\n",
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "from datetime import datetime, timezone\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "# ファイル先頭付近（importの後）に追加\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "for p in (Path(\".env.s3\"), Path(\".env\")):\n",
    "    if p.exists():\n",
    "        load_dotenv(dotenv_path=p, override=False)\n",
    "# ---- 入出力 ----\n",
    "PARQUET_DIR   = Path(\"./data/parquet\")\n",
    "WEIGHT_PARQUET = PARQUET_DIR / \"topixweight_j.parquet\"\n",
    "OUT_PRICES     = PARQUET_DIR / \"core30_prices_1y_1d.parquet\"  # Dashが読み込む価格テーブル\n",
    "OUT_META       = PARQUET_DIR / \"core30_meta.parquet\"          # {code, stock_name, ticker}\n",
    "MANIFEST_PATH  = PARQUET_DIR / \"manifest.json\"\n",
    "\n",
    "# ---- S3 設定（存在すればアップロードを実行）----\n",
    "DATA_BUCKET     = os.getenv(\"DATA_BUCKET\")                     # 例: \"dash-plotly\"\n",
    "PARQUET_PREFIX  = os.getenv(\"PARQUET_PREFIX\", \"parquet/\")      # 例: \"parquet/\"\n",
    "AWS_REGION      = os.getenv(\"AWS_REGION\")\n",
    "AWS_PROFILE     = os.getenv(\"AWS_PROFILE\")   # ~/.aws/credentials を使う場合に指定\n",
    "\n",
    "# ---- 便利関数 ----\n",
    "def _sha256_of(path: Path, chunk_size: int = 1024 * 1024) -> str:\n",
    "    h = hashlib.sha256()\n",
    "    with path.open(\"rb\") as f:\n",
    "        while True:\n",
    "            b = f.read(chunk_size)\n",
    "            if not b:\n",
    "                break\n",
    "            h.update(b)\n",
    "    return h.hexdigest()\n",
    "\n",
    "def _write_manifest_atomic(items: list[dict], path: Path) -> None:\n",
    "    \"\"\"\n",
    "    items: [{\"key\": \"core30_meta.parquet\", \"bytes\": 123, \"sha256\": \"...\", \"mtime\": \"...Z\"}, ...]\n",
    "    \"\"\"\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    payload = {\n",
    "        \"generated_at\": datetime.now(timezone.utc).isoformat(),\n",
    "        \"items\": sorted(items, key=lambda d: d[\"key\"]),\n",
    "        \"note\": \"Auto-generated. Do not edit by hand.\"\n",
    "    }\n",
    "    tmp = path.with_suffix(path.suffix + \".tmp\")\n",
    "    with tmp.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(payload, f, ensure_ascii=False, indent=2)\n",
    "    tmp.replace(path)\n",
    "\n",
    "def _maybe_upload_to_s3(files: list[Path]) -> None:\n",
    "    if not DATA_BUCKET:\n",
    "        print(\"[INFO] DATA_BUCKET 未設定のため S3 アップロードはスキップします。\")\n",
    "        return\n",
    "    try:\n",
    "        import boto3\n",
    "        session_kwargs = {}\n",
    "        if AWS_PROFILE:\n",
    "            session_kwargs[\"profile_name\"] = AWS_PROFILE\n",
    "        session = boto3.Session(**session_kwargs) if session_kwargs else boto3.Session()\n",
    "        s3 = session.client(\"s3\", region_name=AWS_REGION) if AWS_REGION else session.client(\"s3\")\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] boto3 初期化に失敗: {e}  S3アップロードをスキップします。\")\n",
    "        return\n",
    "\n",
    "    for p in files:\n",
    "        key = f\"{PARQUET_PREFIX}{p.name}\"\n",
    "        try:\n",
    "            extra = {\n",
    "                \"ServerSideEncryption\": \"AES256\",\n",
    "                \"CacheControl\": \"max-age=60\",\n",
    "                \"ContentType\": \"application/octet-stream\",\n",
    "            }\n",
    "            s3.upload_file(str(p), DATA_BUCKET, key, ExtraArgs=extra)\n",
    "            print(f\"[OK] uploaded: s3://{DATA_BUCKET}/{key}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] upload failed for {p} → s3://{DATA_BUCKET}/{key}: {e}\")\n",
    "\n",
    "# ---- 読み込み（Core30 抽出）----\n",
    "if not WEIGHT_PARQUET.exists():\n",
    "    raise FileNotFoundError(f\"not found: {WEIGHT_PARQUET}\")\n",
    "\n",
    "w = pd.read_parquet(WEIGHT_PARQUET, engine=\"pyarrow\")\n",
    "\n",
    "# 安全化\n",
    "for col in (\"code\", \"stock_name\", \"size_class\"):\n",
    "    if col not in w.columns:\n",
    "        raise KeyError(f\"required column missing: {col}\")\n",
    "\n",
    "w[\"code\"] = w[\"code\"].astype(\"string\")\n",
    "w[\"size_class\"] = w[\"size_class\"].astype(\"string\")\n",
    "\n",
    "# \"TOPIX Core30\" と \"TOPIX Core 30\" の両方に耐性（空白を除去して包含判定）\n",
    "_mask_core30 = w[\"size_class\"].str.replace(\" \", \"\", regex=False).str.contains(\"Core30\", case=False, na=False)\n",
    "core = (\n",
    "    w.loc[_mask_core30, [\"code\", \"stock_name\"]]\n",
    "     .drop_duplicates(subset=[\"code\"])\n",
    "     .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "def _to_ticker(x: str) -> str:\n",
    "    s = str(x).strip()\n",
    "    return s if s.endswith(\".T\") else f\"{s}.T\"\n",
    "\n",
    "core[\"ticker\"] = core[\"code\"].map(_to_ticker)\n",
    "\n",
    "if core.empty:\n",
    "    raise RuntimeError(\"Core30 list is empty. Check 'size_class' values in topixweight_j.parquet.\")\n",
    "\n",
    "tickers = core[\"ticker\"].tolist()\n",
    "\n",
    "# ---- yfinance 取得（1y,1d）----\n",
    "def _flatten_multi(raw: pd.DataFrame, tickers: list[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    yfinance.download(..., group_by='ticker') の MultiIndex 列を\n",
    "    tidy形式 [date, ticker, Open, High, Low, Close, Volume] に整形。\n",
    "    取得できなかった銘柄は自動スキップ。\n",
    "    \"\"\"\n",
    "    frames = []\n",
    "    if isinstance(raw.columns, pd.MultiIndex):\n",
    "        # 典型: level=0 が ticker, level=1 が OHLCV\n",
    "        for t in tickers:\n",
    "            if t in raw.columns.get_level_values(0):\n",
    "                sub = raw[t].copy()\n",
    "                if sub.empty:\n",
    "                    continue\n",
    "                sub = sub.reset_index()\n",
    "                # index名が \"Date\" / None などケースがあるので正規化\n",
    "                if \"Date\" in sub.columns:\n",
    "                    sub = sub.rename(columns={\"Date\": \"date\"})\n",
    "                elif \"index\" in sub.columns:\n",
    "                    sub = sub.rename(columns={\"index\": \"date\"})\n",
    "                else:\n",
    "                    # 念のため\n",
    "                    sub.columns = [\"date\"] + [c for c in sub.columns[1:]]\n",
    "                sub[\"ticker\"] = t\n",
    "                keep = [c for c in [\"date\",\"Open\",\"High\",\"Low\",\"Close\",\"Volume\",\"ticker\"] if c in sub.columns]\n",
    "                frames.append(sub[keep])\n",
    "    else:\n",
    "        # 単一銘柄や想定外の形の場合：汎用処理\n",
    "        sub = raw.reset_index()\n",
    "        if \"Date\" in sub.columns:\n",
    "            sub = sub.rename(columns={\"Date\": \"date\"})\n",
    "        elif \"index\" in sub.columns:\n",
    "            sub = sub.rename(columns={\"index\": \"date\"})\n",
    "        sub[\"ticker\"] = tickers[0] if tickers else \"UNKNOWN\"\n",
    "        keep = [c for c in [\"date\",\"Open\",\"High\",\"Low\",\"Close\",\"Volume\",\"ticker\"] if c in sub.columns]\n",
    "        frames.append(sub[keep])\n",
    "\n",
    "    if not frames:\n",
    "        return pd.DataFrame(columns=[\"date\",\"Open\",\"High\",\"Low\",\"Close\",\"Volume\",\"ticker\"])\n",
    "    out = pd.concat(frames, ignore_index=True)\n",
    "    # tz-aware → naive への安全化\n",
    "    if np.issubdtype(out[\"date\"].dtype, np.datetime64):\n",
    "        try:\n",
    "            out[\"date\"] = pd.to_datetime(out[\"date\"]).dt.tz_localize(None)\n",
    "        except Exception:\n",
    "            out[\"date\"] = pd.to_datetime(out[\"date\"], utc=True).dt.tz_localize(None)\n",
    "    else:\n",
    "        out[\"date\"] = pd.to_datetime(out[\"date\"], errors=\"coerce\")\n",
    "    # 列型を最小限整える\n",
    "    for c in [\"Open\",\"High\",\"Low\",\"Close\",\"Volume\"]:\n",
    "        if c in out.columns:\n",
    "            out[c] = pd.to_numeric(out[c], errors=\"coerce\")\n",
    "    return out\n",
    "\n",
    "# まとめてダウンロード（失敗時は個別フォールバック）\n",
    "try:\n",
    "    raw = yf.download(\n",
    "        tickers,\n",
    "        period=\"1y\",\n",
    "        interval=\"1d\",\n",
    "        group_by=\"ticker\",\n",
    "        threads=True,\n",
    "        progress=False,\n",
    "    )\n",
    "    prices = _flatten_multi(raw, tickers)\n",
    "    if prices.empty:\n",
    "        raise RuntimeError(\"yf.download returned empty. fallback to per-ticker.\")\n",
    "except Exception:\n",
    "    frames = []\n",
    "    for t in tickers:\n",
    "        try:\n",
    "            r = yf.download(t, period=\"1y\", interval=\"1d\", group_by=\"ticker\", threads=True, progress=False)\n",
    "            f = _flatten_multi(r, [t])\n",
    "            if not f.empty:\n",
    "                frames.append(f)\n",
    "        except Exception:\n",
    "            # 取得失敗ティッカーはスキップ（エラーでNotebook停止させない）\n",
    "            pass\n",
    "    prices = pd.concat(frames, ignore_index=True) if frames else pd.DataFrame()\n",
    "\n",
    "# 必須列がなければ空\n",
    "need = {\"date\",\"Open\",\"High\",\"Low\",\"Close\",\"ticker\"}\n",
    "if prices.empty or not need.issubset(prices.columns):\n",
    "    raise RuntimeError(\"No price data collected or required columns missing.\")\n",
    "\n",
    "# ---- メタ保存 & 価格保存 ----\n",
    "OUT_PRICES.parent.mkdir(parents=True, exist_ok=True)\n",
    "core.to_parquet(OUT_META, engine=\"pyarrow\", index=False)\n",
    "prices.to_parquet(OUT_PRICES, engine=\"pyarrow\", index=False)\n",
    "\n",
    "print(f\"[OK] meta  saved: {OUT_META}   rows={len(core)}\")\n",
    "print(f\"[OK] prices saved: {OUT_PRICES} rows={len(prices)}\")\n",
    "\n",
    "# ---- manifest.json を原子的に作成/更新 ----\n",
    "items = []\n",
    "for p in [OUT_META, OUT_PRICES]:\n",
    "    stat = p.stat()\n",
    "    items.append({\n",
    "        \"key\": p.name,\n",
    "        \"bytes\": stat.st_size,\n",
    "        \"sha256\": _sha256_of(p),\n",
    "        \"mtime\": datetime.fromtimestamp(stat.st_mtime, tz=timezone.utc).isoformat()\n",
    "    })\n",
    "_write_manifest_atomic(items, MANIFEST_PATH)\n",
    "print(f\"[OK] manifest updated: {MANIFEST_PATH}\")\n",
    "\n",
    "# ---- S3 へアップロード（環境変数があれば）----\n",
    "_to_upload = [OUT_META, OUT_PRICES, MANIFEST_PATH]\n",
    "_maybe_upload_to_s3(_to_upload)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a8afa4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
